{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MariaDB Vector Magics - Professional Demo\n",
    "\n",
    "This notebook demonstrates the professional capabilities of MariaDB Vector Magics for semantic search and RAG applications.\n",
    "\n",
    "## Prerequisites\n",
    "- MariaDB 11.7+ with Vector support running\n",
    "- Python environment with mariadb-vector-magics installed\n",
    "- Optional: Groq API key for RAG functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Connection\n",
    "\n",
    "Load the extension and establish connection to MariaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MariaDB Vector Magics extension\n",
    "%load_ext mariadb_vector_magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MariaDB (replace with your credentials)\n",
    "%connect_mariadb --password=your_password --database=vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Embedding Model\n",
    "\n",
    "Load a sentence transformer model for generating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model (this may take a moment on first run)\n",
    "%load_embedding_model all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Vector Store\n",
    "\n",
    "Create an optimized vector table with HNSW indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store for documents\n",
    "%create_vector_store --table tech_docs --dimensions 384 --distance cosine --drop_if_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add Sample Documents\n",
    "\n",
    "Insert sample technology documents with automatic embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%embed_table --table tech_docs --batch_size 16 --chunk_size 500\n",
    "[\n",
    "    \"MariaDB is an open-source relational database management system that originated as a fork of MySQL. It provides enterprise-grade features including ACID compliance, advanced security, and high availability. The recent addition of Vector support enables semantic search capabilities for AI applications.\",\n",
    "    \n",
    "    \"Vector databases are specialized systems designed to store and query high-dimensional vectors efficiently. They use algorithms like HNSW (Hierarchical Navigable Small World) to enable fast approximate nearest neighbor search. This technology is essential for applications involving embeddings, similarity search, and recommendation systems.\",\n",
    "    \n",
    "    \"Retrieval Augmented Generation (RAG) is an AI architecture that combines information retrieval with large language models. The system first retrieves relevant documents from a knowledge base using semantic search, then uses those documents as context for generating accurate, grounded responses. This approach significantly reduces hallucinations in AI systems.\",\n",
    "    \n",
    "    \"Jupyter notebooks provide an interactive computing environment that combines code execution, rich text, and visualizations. They have become the standard tool for data science, machine learning experimentation, and educational content. The notebook format enables reproducible research and collaborative development.\",\n",
    "    \n",
    "    \"Python has emerged as the dominant programming language for artificial intelligence and machine learning. Its extensive ecosystem includes libraries like NumPy for numerical computing, pandas for data manipulation, scikit-learn for machine learning, and PyTorch/TensorFlow for deep learning. The language's simplicity and readability make it accessible to researchers and practitioners.\",\n",
    "    \n",
    "    \"Semantic search goes beyond keyword matching to understand the meaning and context of queries. It uses natural language processing and vector embeddings to find relevant content based on conceptual similarity rather than exact word matches. This technology powers modern search engines, recommendation systems, and question-answering applications.\",\n",
    "    \n",
    "    \"Large Language Models (LLMs) like GPT, Claude, and Llama have revolutionized natural language processing. These models are trained on vast amounts of text data and can perform tasks like text generation, summarization, translation, and question answering. However, they can suffer from hallucinations and lack access to current information, which RAG systems help address.\",\n",
    "    \n",
    "    \"Docker containerization has transformed software deployment by packaging applications with their dependencies into lightweight, portable containers. This technology ensures consistent behavior across different environments and simplifies scaling, orchestration, and continuous integration/continuous deployment (CI/CD) pipelines.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore the Data\n",
    "\n",
    "Check what we've stored in our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all tables in the database\n",
    "%show_vector_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the table to see sample data\n",
    "%query_table tech_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Semantic Search Examples\n",
    "\n",
    "Perform semantic searches to find relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for database-related content\n",
    "%semantic_search \"database systems and storage\" --table tech_docs --top_k 3 --show_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for AI and machine learning content\n",
    "%semantic_search \"artificial intelligence and machine learning\" --table tech_docs --top_k 3 --show_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for development tools\n",
    "%semantic_search \"software development and programming tools\" --table tech_docs --top_k 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with distance threshold filtering\n",
    "%semantic_search \"natural language processing\" --table tech_docs --top_k 5 --threshold 0.7 --show_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RAG (Retrieval Augmented Generation)\n",
    "\n",
    "Demonstrate the complete RAG pipeline with LLM integration.\n",
    "\n",
    "**Note:** You need to set your Groq API key as an environment variable or pass it as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Groq API key (get free key at https://console.groq.com/)\n",
    "import os\n",
    "# os.environ['GROQ_API_KEY'] = 'your_groq_api_key_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions about the stored documents\n",
    "%rag_query \"What is MariaDB and what makes it special?\" --table tech_docs --top_k 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask about RAG systems\n",
    "%rag_query \"How does RAG work and why is it useful?\" --table tech_docs --top_k 3 --temperature 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask about Python for AI\n",
    "%rag_query \"Why is Python popular for machine learning?\" --table tech_docs --top_k 2 --model llama-3.1-70b-versatile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Use Cases\n",
    "\n",
    "Demonstrate more advanced scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a specialized vector store for code documentation\n",
    "%create_vector_store --table code_docs --dimensions 384 --distance euclidean --m_value 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%embed_table --table code_docs --batch_size 8 --chunk_size 300\n",
    "[\n",
    "    \"The connect_mariadb magic command establishes a connection to MariaDB server. Parameters include host, port, user, password, and database. It validates Vector support and displays connection status.\",\n",
    "    \n",
    "    \"The load_embedding_model command loads a sentence transformer model for generating embeddings. Default model is all-MiniLM-L6-v2 with 384 dimensions. Models are cached locally for faster subsequent loads.\",\n",
    "    \n",
    "    \"The create_vector_store command creates optimized tables for vector storage. It supports HNSW indexing with configurable M values, distance metrics (cosine/euclidean), and automatic schema generation.\",\n",
    "    \n",
    "    \"The embed_table cell magic processes documents in batches, generates embeddings, and inserts them into vector tables. It supports automatic chunking for large documents with configurable overlap.\",\n",
    "    \n",
    "    \"The semantic_search command performs similarity search using vector embeddings. It supports distance thresholds, result limiting, and displays similarity scores when requested.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the code documentation\n",
    "%semantic_search \"how to connect to database\" --table code_docs --top_k 2 --show_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance and Monitoring\n",
    "\n",
    "Monitor the performance of your vector operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check both tables\n",
    "%query_table tech_docs\n",
    "%query_table code_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup (Optional)\n",
    "\n",
    "Clean up the demo data if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to drop the demo tables\n",
    "# %create_vector_store --table tech_docs --dimensions 384 --drop_if_exists\n",
    "# %create_vector_store --table code_docs --dimensions 384 --drop_if_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This demo showcased the key features of MariaDB Vector Magics:\n",
    "\n",
    "1. **Easy Setup**: Simple magic commands for connection and configuration\n",
    "2. **Automatic Embeddings**: Seamless text-to-vector conversion\n",
    "3. **Optimized Storage**: HNSW-indexed vector tables for fast search\n",
    "4. **Semantic Search**: Natural language queries with similarity scoring\n",
    "5. **RAG Integration**: Complete retrieval-augmented generation pipeline\n",
    "6. **Professional Features**: Batch processing, chunking, and error handling\n",
    "\n",
    "For production use, consider:\n",
    "- Using environment variables for credentials\n",
    "- Implementing proper error handling\n",
    "- Monitoring performance and scaling\n",
    "- Setting up automated backups\n",
    "- Configuring appropriate security measures\n",
    "\n",
    "Visit the [GitHub repository](https://github.com/jayant99acharya/mariadb) for more information and updates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
